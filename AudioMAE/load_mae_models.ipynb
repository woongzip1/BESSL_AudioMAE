{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woongjib/anaconda3/envs/env3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unexpected keys (norm.weight, norm.bias) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [1, 513, 768]             396,288\n",
       "├─PatchEmbed: 1-1                        [1, 512, 768]             --\n",
       "│    └─Conv2d: 2-1                       [1, 768, 64, 8]           197,376\n",
       "│    └─Identity: 2-2                     [1, 512, 768]             --\n",
       "├─Dropout: 1-2                           [1, 513, 768]             --\n",
       "├─Identity: 1-3                          [1, 513, 768]             --\n",
       "├─Identity: 1-4                          [1, 513, 768]             --\n",
       "├─Sequential: 1-5                        [1, 513, 768]             --\n",
       "│    └─Block: 2-3                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-1               [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-2               [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-3                [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-4                [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-5               [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-6                     [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-7                [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-8                [1, 513, 768]             --\n",
       "│    └─Block: 2-4                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-10              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-11               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-12               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-13              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-14                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-15               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-16               [1, 513, 768]             --\n",
       "│    └─Block: 2-5                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-17              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-18              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-19               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-20               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-22                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-23               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-24               [1, 513, 768]             --\n",
       "│    └─Block: 2-6                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-25              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-26              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-27               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-28               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-29              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-30                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-31               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-32               [1, 513, 768]             --\n",
       "│    └─Block: 2-7                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-34              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-35               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-36               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-37              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-38                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-39               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-40               [1, 513, 768]             --\n",
       "│    └─Block: 2-8                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-41              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-42              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-43               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-44               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-45              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-46                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-47               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-48               [1, 513, 768]             --\n",
       "│    └─Block: 2-9                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-49              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-50              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-51               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-52               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-53              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-54                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-55               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-56               [1, 513, 768]             --\n",
       "│    └─Block: 2-10                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-57              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-58              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-59               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-60               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-61              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-62                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-63               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-64               [1, 513, 768]             --\n",
       "│    └─Block: 2-11                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-65              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-66              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-67               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-68               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-69              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-70                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-71               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-72               [1, 513, 768]             --\n",
       "│    └─Block: 2-12                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-73              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-74              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-75               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-76               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-77              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-78                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-79               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-80               [1, 513, 768]             --\n",
       "│    └─Block: 2-13                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-81              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-82              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-83               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-84               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-85              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-86                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-87               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-88               [1, 513, 768]             --\n",
       "│    └─Block: 2-14                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-89              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-90              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-91               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-92               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-93              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-94                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-95               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-96               [1, 513, 768]             --\n",
       "├─Identity: 1-6                          [1, 513, 768]             --\n",
       "==========================================================================================\n",
       "Total params: 85,648,128\n",
       "Trainable params: 85,648,128\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 186.11\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 419.19\n",
       "Params size (MB): 341.01\n",
       "Estimated Total Size (MB): 760.72\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\"\"\"\n",
    "Hugging Face TIMM Load\n",
    "\"\"\"\n",
    "\n",
    "# model = timm.create_model('resnet18', pretrained=True)\n",
    "\n",
    "model = timm.create_model('hf_hub:gaunernst/vit_base_patch16_1024_128.audiomae_as2m', \n",
    "                            num_classes=10,\n",
    "                            global_pool='avg',\n",
    "                            pretrained=True,)\n",
    "del model.head_drop\n",
    "del model.head\n",
    "model.forward = model.forward_features\n",
    "print(model)\n",
    "\n",
    "\n",
    "data = torch.rand(1,1,1024,128)\n",
    "summary(model, input_data = data)\n",
    "# print(model(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedAutoencoderViT(\n",
      "  (patch_embed): PatchEmbed_org(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "Befor Projection torch.Size([1, 1, 1024, 128])\n",
      "After Projection torch.Size([1, 768, 64, 8])\n",
      "torch.Size([1, 513, 768])\n",
      "Befor Projection torch.Size([1, 1, 1024, 128])\n",
      "After Projection torch.Size([1, 768, 64, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MaskedAutoencoderViT                     [1, 513, 768]             395,264\n",
       "├─PatchEmbed_org: 1-1                    [1, 512, 768]             --\n",
       "│    └─Conv2d: 2-1                       [1, 768, 64, 8]           197,376\n",
       "├─ModuleList: 1-6                        --                        (recursive)\n",
       "│    └─Block: 2-2                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-1               [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-2               [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-3                [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-4                [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-5               [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-6                     [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-7                [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-8                [1, 513, 768]             --\n",
       "│    └─Block: 2-3                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-10              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-11               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-12               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-13              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-14                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-15               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-16               [1, 513, 768]             --\n",
       "│    └─Block: 2-4                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-17              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-18              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-19               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-20               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-22                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-23               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-24               [1, 513, 768]             --\n",
       "│    └─Block: 2-5                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-25              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-26              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-27               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-28               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-29              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-30                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-31               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-32               [1, 513, 768]             --\n",
       "│    └─Block: 2-6                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-34              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-35               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-36               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-37              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-38                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-39               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-40               [1, 513, 768]             --\n",
       "│    └─Block: 2-7                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-41              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-42              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-43               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-44               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-45              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-46                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-47               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-48               [1, 513, 768]             --\n",
       "│    └─Block: 2-8                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-49              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-50              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-51               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-52               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-53              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-54                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-55               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-56               [1, 513, 768]             --\n",
       "│    └─Block: 2-9                        [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-57              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-58              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-59               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-60               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-61              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-62                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-63               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-64               [1, 513, 768]             --\n",
       "│    └─Block: 2-10                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-65              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-66              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-67               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-68               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-69              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-70                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-71               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-72               [1, 513, 768]             --\n",
       "│    └─Block: 2-11                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-73              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-74              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-75               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-76               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-77              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-78                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-79               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-80               [1, 513, 768]             --\n",
       "├─LayerNorm: 1-3                         [1, 513, 768]             1,536\n",
       "├─ModuleList: 1-6                        --                        (recursive)\n",
       "│    └─Block: 2-12                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-81              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-82              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-83               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-84               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-85              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-86                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-87               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-88               [1, 513, 768]             --\n",
       "├─LayerNorm: 1-5                         [1, 513, 768]             (recursive)\n",
       "├─ModuleList: 1-6                        --                        (recursive)\n",
       "│    └─Block: 2-13                       [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-89              [1, 513, 768]             1,536\n",
       "│    │    └─Attention: 3-90              [1, 513, 768]             2,362,368\n",
       "│    │    └─Identity: 3-91               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-92               [1, 513, 768]             --\n",
       "│    │    └─LayerNorm: 3-93              [1, 513, 768]             1,536\n",
       "│    │    └─Mlp: 3-94                    [1, 513, 768]             4,722,432\n",
       "│    │    └─Identity: 3-95               [1, 513, 768]             --\n",
       "│    │    └─Identity: 3-96               [1, 513, 768]             --\n",
       "├─LayerNorm: 1-7                         [1, 513, 768]             (recursive)\n",
       "==========================================================================================\n",
       "Total params: 85,648,640\n",
       "Trainable params: 85,254,656\n",
       "Non-trainable params: 393,984\n",
       "Total mult-adds (Units.MEGABYTES): 186.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 428.65\n",
       "Params size (MB): 341.01\n",
       "Estimated Total Size (MB): 770.19\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models_mae import mae_vit_base_patch16\n",
    "\n",
    "\"\"\" When Utilizing MAE-VIT Encoder \"\"\"\n",
    "model = mae_vit_base_patch16(img_size=[1024,128], audio_exp=True, in_chans=1)\n",
    "del model.decoder_embed\n",
    "del model.decoder_blocks\n",
    "del model.decoder_norm\n",
    "del model.decoder_pred\n",
    "del model.log_softmax\n",
    "del model.decoder_pos_embed\n",
    "model.forward = model.forward_encoder_no_mask\n",
    "print(model)\n",
    "\n",
    "data = torch.rand(1,1,1024,128)\n",
    "out = model(data)\n",
    "print(out.shape)\n",
    "summary(model, input_data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models_vit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" Rougly Utilize VIT Base \"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels_vit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionTransformer, vit_base_patch16\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionTransformer, vit_base_patch16_224 \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models_vit'"
     ]
    }
   ],
   "source": [
    "# \"\"\" Rougly Utilize VIT Base \"\"\"\n",
    "# from models_vit import VisionTransformer, vit_base_patch16\n",
    "# from timm.models import VisionTransformer, vit_base_patch16_224 \n",
    "# import torch\n",
    "\n",
    "# # model = vit_base_patch16_224(img_size=[1024,128], global_pool='avg', in_chans=1)\n",
    "# model = vit_base_patch16(img_size=[1024,128], global_pool='avg', in_chans=1)\n",
    "# del model.head\n",
    "# del model.head_drop\n",
    "# model.forward = model.forward_features\n",
    "# print(model)\n",
    "\n",
    "# data = torch.rand(1,1,1024,128)\n",
    "# summary(model, input_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args'])\n",
      "Namespace(batch_size=8, epochs=33, accum_iter=1, model='mae_vit_base_patch16', input_size=224, mask_ratio=0.8, norm_pix_loss=True, weight_decay=0.0001, lr=0.0004, blr=0.0002, min_lr=0.0, warmup_epochs=3, data_path='/datasets01/imagenet_full_size/061417/', output_dir=PosixPath('/checkpoint/berniehuang/experiments/56067384'), log_dir=PosixPath('/checkpoint/berniehuang/experiments/56067384'), device='cuda', seed=0, resume='', start_epoch=0, num_workers=10, pin_mem=True, world_size=64, local_rank=-1, dist_on_itp=False, dist_url='file:///checkpoint/berniehuang/experiments/674c1c0b0a3e4b54a9d7722b93d3b300_init', audio_exp=True, data_train='/checkpoint/berniehuang/ast/egs/audioset/data/datafiles/train_all.json', data_eval='/checkpoint/berniehuang/ast/egs/audioset/data/datafiles/eval_video.json', label_csv='/checkpoint/berniehuang/ast/egs/audioset/data/class_labels_indices.csv', freqm=0, timem=0, mixup=0, dataset='audioset', use_fbank=False, fbank_dir='/checkpoint/berniehuang/ast/egs/esc50/data/ESC-50-master/fbank', alpha=0.0, omega=1.0, mode=0, save_every_epoch=4, use_custom_patch=False, roll_mag_aug=True, split_pos=False, pos_trainable=False, use_nce=False, img_pt_ckpt='', load_video=False, sep_encoder=False, encoder_mode=0, decoder_mode=1, n_frm=6, depth_av=6, v_weight=1.0, mask_t_prob=0.7, mask_f_prob=0.3, mask_2d=False, video_only=False, cl=False, use_video_mae=False, ngpus=8, nodes=8, timeout=4320, job_dir=PosixPath('/checkpoint/berniehuang/experiments/%j'), partition='learnlab', use_volta32=True, comment='', gpu=0, rank=0, distributed=True, dist_backend='nccl')\n",
      "odict_keys(['cls_token', 'pos_embed', 'mask_token', 'decoder_pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.attn.tau', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.attn.meta_mlp.fc1.weight', 'decoder_blocks.0.attn.meta_mlp.fc1.bias', 'decoder_blocks.0.attn.meta_mlp.fc2.weight', 'decoder_blocks.0.attn.meta_mlp.fc2.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.1.attn.tau', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.attn.meta_mlp.fc1.weight', 'decoder_blocks.1.attn.meta_mlp.fc1.bias', 'decoder_blocks.1.attn.meta_mlp.fc2.weight', 'decoder_blocks.1.attn.meta_mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.2.attn.tau', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.attn.meta_mlp.fc1.weight', 'decoder_blocks.2.attn.meta_mlp.fc1.bias', 'decoder_blocks.2.attn.meta_mlp.fc2.weight', 'decoder_blocks.2.attn.meta_mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.3.attn.tau', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.attn.meta_mlp.fc1.weight', 'decoder_blocks.3.attn.meta_mlp.fc1.bias', 'decoder_blocks.3.attn.meta_mlp.fc2.weight', 'decoder_blocks.3.attn.meta_mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.4.attn.tau', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.attn.meta_mlp.fc1.weight', 'decoder_blocks.4.attn.meta_mlp.fc1.bias', 'decoder_blocks.4.attn.meta_mlp.fc2.weight', 'decoder_blocks.4.attn.meta_mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.5.attn.tau', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.attn.meta_mlp.fc1.weight', 'decoder_blocks.5.attn.meta_mlp.fc1.bias', 'decoder_blocks.5.attn.meta_mlp.fc2.weight', 'decoder_blocks.5.attn.meta_mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.6.attn.tau', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.attn.meta_mlp.fc1.weight', 'decoder_blocks.6.attn.meta_mlp.fc1.bias', 'decoder_blocks.6.attn.meta_mlp.fc2.weight', 'decoder_blocks.6.attn.meta_mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.7.attn.tau', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.attn.meta_mlp.fc1.weight', 'decoder_blocks.7.attn.meta_mlp.fc1.bias', 'decoder_blocks.7.attn.meta_mlp.fc2.weight', 'decoder_blocks.7.attn.meta_mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.8.attn.tau', 'decoder_blocks.8.attn.qkv.weight', 'decoder_blocks.8.attn.qkv.bias', 'decoder_blocks.8.attn.proj.weight', 'decoder_blocks.8.attn.proj.bias', 'decoder_blocks.8.attn.meta_mlp.fc1.weight', 'decoder_blocks.8.attn.meta_mlp.fc1.bias', 'decoder_blocks.8.attn.meta_mlp.fc2.weight', 'decoder_blocks.8.attn.meta_mlp.fc2.bias', 'decoder_blocks.8.norm1.weight', 'decoder_blocks.8.norm1.bias', 'decoder_blocks.8.mlp.fc1.weight', 'decoder_blocks.8.mlp.fc1.bias', 'decoder_blocks.8.mlp.fc2.weight', 'decoder_blocks.8.mlp.fc2.bias', 'decoder_blocks.8.norm2.weight', 'decoder_blocks.8.norm2.bias', 'decoder_blocks.9.attn.tau', 'decoder_blocks.9.attn.qkv.weight', 'decoder_blocks.9.attn.qkv.bias', 'decoder_blocks.9.attn.proj.weight', 'decoder_blocks.9.attn.proj.bias', 'decoder_blocks.9.attn.meta_mlp.fc1.weight', 'decoder_blocks.9.attn.meta_mlp.fc1.bias', 'decoder_blocks.9.attn.meta_mlp.fc2.weight', 'decoder_blocks.9.attn.meta_mlp.fc2.bias', 'decoder_blocks.9.norm1.weight', 'decoder_blocks.9.norm1.bias', 'decoder_blocks.9.mlp.fc1.weight', 'decoder_blocks.9.mlp.fc1.bias', 'decoder_blocks.9.mlp.fc2.weight', 'decoder_blocks.9.mlp.fc2.bias', 'decoder_blocks.9.norm2.weight', 'decoder_blocks.9.norm2.bias', 'decoder_blocks.10.attn.tau', 'decoder_blocks.10.attn.qkv.weight', 'decoder_blocks.10.attn.qkv.bias', 'decoder_blocks.10.attn.proj.weight', 'decoder_blocks.10.attn.proj.bias', 'decoder_blocks.10.attn.meta_mlp.fc1.weight', 'decoder_blocks.10.attn.meta_mlp.fc1.bias', 'decoder_blocks.10.attn.meta_mlp.fc2.weight', 'decoder_blocks.10.attn.meta_mlp.fc2.bias', 'decoder_blocks.10.norm1.weight', 'decoder_blocks.10.norm1.bias', 'decoder_blocks.10.mlp.fc1.weight', 'decoder_blocks.10.mlp.fc1.bias', 'decoder_blocks.10.mlp.fc2.weight', 'decoder_blocks.10.mlp.fc2.bias', 'decoder_blocks.10.norm2.weight', 'decoder_blocks.10.norm2.bias', 'decoder_blocks.11.attn.tau', 'decoder_blocks.11.attn.qkv.weight', 'decoder_blocks.11.attn.qkv.bias', 'decoder_blocks.11.attn.proj.weight', 'decoder_blocks.11.attn.proj.bias', 'decoder_blocks.11.attn.meta_mlp.fc1.weight', 'decoder_blocks.11.attn.meta_mlp.fc1.bias', 'decoder_blocks.11.attn.meta_mlp.fc2.weight', 'decoder_blocks.11.attn.meta_mlp.fc2.bias', 'decoder_blocks.11.norm1.weight', 'decoder_blocks.11.norm1.bias', 'decoder_blocks.11.mlp.fc1.weight', 'decoder_blocks.11.mlp.fc1.bias', 'decoder_blocks.11.mlp.fc2.weight', 'decoder_blocks.11.mlp.fc2.bias', 'decoder_blocks.11.norm2.weight', 'decoder_blocks.11.norm2.bias', 'decoder_blocks.12.attn.tau', 'decoder_blocks.12.attn.qkv.weight', 'decoder_blocks.12.attn.qkv.bias', 'decoder_blocks.12.attn.proj.weight', 'decoder_blocks.12.attn.proj.bias', 'decoder_blocks.12.attn.meta_mlp.fc1.weight', 'decoder_blocks.12.attn.meta_mlp.fc1.bias', 'decoder_blocks.12.attn.meta_mlp.fc2.weight', 'decoder_blocks.12.attn.meta_mlp.fc2.bias', 'decoder_blocks.12.norm1.weight', 'decoder_blocks.12.norm1.bias', 'decoder_blocks.12.mlp.fc1.weight', 'decoder_blocks.12.mlp.fc1.bias', 'decoder_blocks.12.mlp.fc2.weight', 'decoder_blocks.12.mlp.fc2.bias', 'decoder_blocks.12.norm2.weight', 'decoder_blocks.12.norm2.bias', 'decoder_blocks.13.attn.tau', 'decoder_blocks.13.attn.qkv.weight', 'decoder_blocks.13.attn.qkv.bias', 'decoder_blocks.13.attn.proj.weight', 'decoder_blocks.13.attn.proj.bias', 'decoder_blocks.13.attn.meta_mlp.fc1.weight', 'decoder_blocks.13.attn.meta_mlp.fc1.bias', 'decoder_blocks.13.attn.meta_mlp.fc2.weight', 'decoder_blocks.13.attn.meta_mlp.fc2.bias', 'decoder_blocks.13.norm1.weight', 'decoder_blocks.13.norm1.bias', 'decoder_blocks.13.mlp.fc1.weight', 'decoder_blocks.13.mlp.fc1.bias', 'decoder_blocks.13.mlp.fc2.weight', 'decoder_blocks.13.mlp.fc2.bias', 'decoder_blocks.13.norm2.weight', 'decoder_blocks.13.norm2.bias', 'decoder_blocks.14.attn.tau', 'decoder_blocks.14.attn.qkv.weight', 'decoder_blocks.14.attn.qkv.bias', 'decoder_blocks.14.attn.proj.weight', 'decoder_blocks.14.attn.proj.bias', 'decoder_blocks.14.attn.meta_mlp.fc1.weight', 'decoder_blocks.14.attn.meta_mlp.fc1.bias', 'decoder_blocks.14.attn.meta_mlp.fc2.weight', 'decoder_blocks.14.attn.meta_mlp.fc2.bias', 'decoder_blocks.14.norm1.weight', 'decoder_blocks.14.norm1.bias', 'decoder_blocks.14.mlp.fc1.weight', 'decoder_blocks.14.mlp.fc1.bias', 'decoder_blocks.14.mlp.fc2.weight', 'decoder_blocks.14.mlp.fc2.bias', 'decoder_blocks.14.norm2.weight', 'decoder_blocks.14.norm2.bias', 'decoder_blocks.15.attn.tau', 'decoder_blocks.15.attn.qkv.weight', 'decoder_blocks.15.attn.qkv.bias', 'decoder_blocks.15.attn.proj.weight', 'decoder_blocks.15.attn.proj.bias', 'decoder_blocks.15.attn.meta_mlp.fc1.weight', 'decoder_blocks.15.attn.meta_mlp.fc1.bias', 'decoder_blocks.15.attn.meta_mlp.fc2.weight', 'decoder_blocks.15.attn.meta_mlp.fc2.bias', 'decoder_blocks.15.norm1.weight', 'decoder_blocks.15.norm1.bias', 'decoder_blocks.15.mlp.fc1.weight', 'decoder_blocks.15.mlp.fc1.bias', 'decoder_blocks.15.mlp.fc2.weight', 'decoder_blocks.15.mlp.fc2.bias', 'decoder_blocks.15.norm2.weight', 'decoder_blocks.15.norm2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])\n",
      "\n",
      " ****** Unused Parameters ****** \n",
      "\n",
      "decoder_pos_embed\n",
      "decoder_embed.weight\n",
      "decoder_embed.bias\n",
      "decoder_blocks.0.attn.tau\n",
      "decoder_blocks.0.attn.qkv.weight\n",
      "decoder_blocks.0.attn.qkv.bias\n",
      "decoder_blocks.0.attn.proj.weight\n",
      "decoder_blocks.0.attn.proj.bias\n",
      "decoder_blocks.0.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.0.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.0.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.0.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.0.norm1.weight\n",
      "decoder_blocks.0.norm1.bias\n",
      "decoder_blocks.0.mlp.fc1.weight\n",
      "decoder_blocks.0.mlp.fc1.bias\n",
      "decoder_blocks.0.mlp.fc2.weight\n",
      "decoder_blocks.0.mlp.fc2.bias\n",
      "decoder_blocks.0.norm2.weight\n",
      "decoder_blocks.0.norm2.bias\n",
      "decoder_blocks.1.attn.tau\n",
      "decoder_blocks.1.attn.qkv.weight\n",
      "decoder_blocks.1.attn.qkv.bias\n",
      "decoder_blocks.1.attn.proj.weight\n",
      "decoder_blocks.1.attn.proj.bias\n",
      "decoder_blocks.1.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.1.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.1.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.1.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.1.norm1.weight\n",
      "decoder_blocks.1.norm1.bias\n",
      "decoder_blocks.1.mlp.fc1.weight\n",
      "decoder_blocks.1.mlp.fc1.bias\n",
      "decoder_blocks.1.mlp.fc2.weight\n",
      "decoder_blocks.1.mlp.fc2.bias\n",
      "decoder_blocks.1.norm2.weight\n",
      "decoder_blocks.1.norm2.bias\n",
      "decoder_blocks.2.attn.tau\n",
      "decoder_blocks.2.attn.qkv.weight\n",
      "decoder_blocks.2.attn.qkv.bias\n",
      "decoder_blocks.2.attn.proj.weight\n",
      "decoder_blocks.2.attn.proj.bias\n",
      "decoder_blocks.2.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.2.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.2.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.2.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.2.norm1.weight\n",
      "decoder_blocks.2.norm1.bias\n",
      "decoder_blocks.2.mlp.fc1.weight\n",
      "decoder_blocks.2.mlp.fc1.bias\n",
      "decoder_blocks.2.mlp.fc2.weight\n",
      "decoder_blocks.2.mlp.fc2.bias\n",
      "decoder_blocks.2.norm2.weight\n",
      "decoder_blocks.2.norm2.bias\n",
      "decoder_blocks.3.attn.tau\n",
      "decoder_blocks.3.attn.qkv.weight\n",
      "decoder_blocks.3.attn.qkv.bias\n",
      "decoder_blocks.3.attn.proj.weight\n",
      "decoder_blocks.3.attn.proj.bias\n",
      "decoder_blocks.3.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.3.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.3.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.3.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.3.norm1.weight\n",
      "decoder_blocks.3.norm1.bias\n",
      "decoder_blocks.3.mlp.fc1.weight\n",
      "decoder_blocks.3.mlp.fc1.bias\n",
      "decoder_blocks.3.mlp.fc2.weight\n",
      "decoder_blocks.3.mlp.fc2.bias\n",
      "decoder_blocks.3.norm2.weight\n",
      "decoder_blocks.3.norm2.bias\n",
      "decoder_blocks.4.attn.tau\n",
      "decoder_blocks.4.attn.qkv.weight\n",
      "decoder_blocks.4.attn.qkv.bias\n",
      "decoder_blocks.4.attn.proj.weight\n",
      "decoder_blocks.4.attn.proj.bias\n",
      "decoder_blocks.4.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.4.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.4.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.4.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.4.norm1.weight\n",
      "decoder_blocks.4.norm1.bias\n",
      "decoder_blocks.4.mlp.fc1.weight\n",
      "decoder_blocks.4.mlp.fc1.bias\n",
      "decoder_blocks.4.mlp.fc2.weight\n",
      "decoder_blocks.4.mlp.fc2.bias\n",
      "decoder_blocks.4.norm2.weight\n",
      "decoder_blocks.4.norm2.bias\n",
      "decoder_blocks.5.attn.tau\n",
      "decoder_blocks.5.attn.qkv.weight\n",
      "decoder_blocks.5.attn.qkv.bias\n",
      "decoder_blocks.5.attn.proj.weight\n",
      "decoder_blocks.5.attn.proj.bias\n",
      "decoder_blocks.5.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.5.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.5.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.5.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.5.norm1.weight\n",
      "decoder_blocks.5.norm1.bias\n",
      "decoder_blocks.5.mlp.fc1.weight\n",
      "decoder_blocks.5.mlp.fc1.bias\n",
      "decoder_blocks.5.mlp.fc2.weight\n",
      "decoder_blocks.5.mlp.fc2.bias\n",
      "decoder_blocks.5.norm2.weight\n",
      "decoder_blocks.5.norm2.bias\n",
      "decoder_blocks.6.attn.tau\n",
      "decoder_blocks.6.attn.qkv.weight\n",
      "decoder_blocks.6.attn.qkv.bias\n",
      "decoder_blocks.6.attn.proj.weight\n",
      "decoder_blocks.6.attn.proj.bias\n",
      "decoder_blocks.6.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.6.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.6.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.6.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.6.norm1.weight\n",
      "decoder_blocks.6.norm1.bias\n",
      "decoder_blocks.6.mlp.fc1.weight\n",
      "decoder_blocks.6.mlp.fc1.bias\n",
      "decoder_blocks.6.mlp.fc2.weight\n",
      "decoder_blocks.6.mlp.fc2.bias\n",
      "decoder_blocks.6.norm2.weight\n",
      "decoder_blocks.6.norm2.bias\n",
      "decoder_blocks.7.attn.tau\n",
      "decoder_blocks.7.attn.qkv.weight\n",
      "decoder_blocks.7.attn.qkv.bias\n",
      "decoder_blocks.7.attn.proj.weight\n",
      "decoder_blocks.7.attn.proj.bias\n",
      "decoder_blocks.7.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.7.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.7.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.7.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.7.norm1.weight\n",
      "decoder_blocks.7.norm1.bias\n",
      "decoder_blocks.7.mlp.fc1.weight\n",
      "decoder_blocks.7.mlp.fc1.bias\n",
      "decoder_blocks.7.mlp.fc2.weight\n",
      "decoder_blocks.7.mlp.fc2.bias\n",
      "decoder_blocks.7.norm2.weight\n",
      "decoder_blocks.7.norm2.bias\n",
      "decoder_blocks.8.attn.tau\n",
      "decoder_blocks.8.attn.qkv.weight\n",
      "decoder_blocks.8.attn.qkv.bias\n",
      "decoder_blocks.8.attn.proj.weight\n",
      "decoder_blocks.8.attn.proj.bias\n",
      "decoder_blocks.8.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.8.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.8.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.8.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.8.norm1.weight\n",
      "decoder_blocks.8.norm1.bias\n",
      "decoder_blocks.8.mlp.fc1.weight\n",
      "decoder_blocks.8.mlp.fc1.bias\n",
      "decoder_blocks.8.mlp.fc2.weight\n",
      "decoder_blocks.8.mlp.fc2.bias\n",
      "decoder_blocks.8.norm2.weight\n",
      "decoder_blocks.8.norm2.bias\n",
      "decoder_blocks.9.attn.tau\n",
      "decoder_blocks.9.attn.qkv.weight\n",
      "decoder_blocks.9.attn.qkv.bias\n",
      "decoder_blocks.9.attn.proj.weight\n",
      "decoder_blocks.9.attn.proj.bias\n",
      "decoder_blocks.9.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.9.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.9.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.9.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.9.norm1.weight\n",
      "decoder_blocks.9.norm1.bias\n",
      "decoder_blocks.9.mlp.fc1.weight\n",
      "decoder_blocks.9.mlp.fc1.bias\n",
      "decoder_blocks.9.mlp.fc2.weight\n",
      "decoder_blocks.9.mlp.fc2.bias\n",
      "decoder_blocks.9.norm2.weight\n",
      "decoder_blocks.9.norm2.bias\n",
      "decoder_blocks.10.attn.tau\n",
      "decoder_blocks.10.attn.qkv.weight\n",
      "decoder_blocks.10.attn.qkv.bias\n",
      "decoder_blocks.10.attn.proj.weight\n",
      "decoder_blocks.10.attn.proj.bias\n",
      "decoder_blocks.10.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.10.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.10.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.10.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.10.norm1.weight\n",
      "decoder_blocks.10.norm1.bias\n",
      "decoder_blocks.10.mlp.fc1.weight\n",
      "decoder_blocks.10.mlp.fc1.bias\n",
      "decoder_blocks.10.mlp.fc2.weight\n",
      "decoder_blocks.10.mlp.fc2.bias\n",
      "decoder_blocks.10.norm2.weight\n",
      "decoder_blocks.10.norm2.bias\n",
      "decoder_blocks.11.attn.tau\n",
      "decoder_blocks.11.attn.qkv.weight\n",
      "decoder_blocks.11.attn.qkv.bias\n",
      "decoder_blocks.11.attn.proj.weight\n",
      "decoder_blocks.11.attn.proj.bias\n",
      "decoder_blocks.11.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.11.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.11.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.11.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.11.norm1.weight\n",
      "decoder_blocks.11.norm1.bias\n",
      "decoder_blocks.11.mlp.fc1.weight\n",
      "decoder_blocks.11.mlp.fc1.bias\n",
      "decoder_blocks.11.mlp.fc2.weight\n",
      "decoder_blocks.11.mlp.fc2.bias\n",
      "decoder_blocks.11.norm2.weight\n",
      "decoder_blocks.11.norm2.bias\n",
      "decoder_blocks.12.attn.tau\n",
      "decoder_blocks.12.attn.qkv.weight\n",
      "decoder_blocks.12.attn.qkv.bias\n",
      "decoder_blocks.12.attn.proj.weight\n",
      "decoder_blocks.12.attn.proj.bias\n",
      "decoder_blocks.12.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.12.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.12.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.12.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.12.norm1.weight\n",
      "decoder_blocks.12.norm1.bias\n",
      "decoder_blocks.12.mlp.fc1.weight\n",
      "decoder_blocks.12.mlp.fc1.bias\n",
      "decoder_blocks.12.mlp.fc2.weight\n",
      "decoder_blocks.12.mlp.fc2.bias\n",
      "decoder_blocks.12.norm2.weight\n",
      "decoder_blocks.12.norm2.bias\n",
      "decoder_blocks.13.attn.tau\n",
      "decoder_blocks.13.attn.qkv.weight\n",
      "decoder_blocks.13.attn.qkv.bias\n",
      "decoder_blocks.13.attn.proj.weight\n",
      "decoder_blocks.13.attn.proj.bias\n",
      "decoder_blocks.13.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.13.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.13.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.13.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.13.norm1.weight\n",
      "decoder_blocks.13.norm1.bias\n",
      "decoder_blocks.13.mlp.fc1.weight\n",
      "decoder_blocks.13.mlp.fc1.bias\n",
      "decoder_blocks.13.mlp.fc2.weight\n",
      "decoder_blocks.13.mlp.fc2.bias\n",
      "decoder_blocks.13.norm2.weight\n",
      "decoder_blocks.13.norm2.bias\n",
      "decoder_blocks.14.attn.tau\n",
      "decoder_blocks.14.attn.qkv.weight\n",
      "decoder_blocks.14.attn.qkv.bias\n",
      "decoder_blocks.14.attn.proj.weight\n",
      "decoder_blocks.14.attn.proj.bias\n",
      "decoder_blocks.14.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.14.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.14.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.14.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.14.norm1.weight\n",
      "decoder_blocks.14.norm1.bias\n",
      "decoder_blocks.14.mlp.fc1.weight\n",
      "decoder_blocks.14.mlp.fc1.bias\n",
      "decoder_blocks.14.mlp.fc2.weight\n",
      "decoder_blocks.14.mlp.fc2.bias\n",
      "decoder_blocks.14.norm2.weight\n",
      "decoder_blocks.14.norm2.bias\n",
      "decoder_blocks.15.attn.tau\n",
      "decoder_blocks.15.attn.qkv.weight\n",
      "decoder_blocks.15.attn.qkv.bias\n",
      "decoder_blocks.15.attn.proj.weight\n",
      "decoder_blocks.15.attn.proj.bias\n",
      "decoder_blocks.15.attn.meta_mlp.fc1.weight\n",
      "decoder_blocks.15.attn.meta_mlp.fc1.bias\n",
      "decoder_blocks.15.attn.meta_mlp.fc2.weight\n",
      "decoder_blocks.15.attn.meta_mlp.fc2.bias\n",
      "decoder_blocks.15.norm1.weight\n",
      "decoder_blocks.15.norm1.bias\n",
      "decoder_blocks.15.mlp.fc1.weight\n",
      "decoder_blocks.15.mlp.fc1.bias\n",
      "decoder_blocks.15.mlp.fc2.weight\n",
      "decoder_blocks.15.mlp.fc2.bias\n",
      "decoder_blocks.15.norm2.weight\n",
      "decoder_blocks.15.norm2.bias\n",
      "decoder_norm.weight\n",
      "decoder_norm.bias\n",
      "decoder_pred.weight\n",
      "decoder_pred.bias\n",
      "\n",
      " ****** Used Parameters ****** \n",
      "\n",
      "cls_token\n",
      "pos_embed\n",
      "mask_token\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = model.state_dict()\n",
    "checkpoint_dict = torch.load(\"/home/woongjib/Projects/BESSL/AudioMAE/pretrained_AS2M.pth\")\n",
    "print(checkpoint_dict.keys())\n",
    "print(checkpoint_dict['args'])\n",
    "print(checkpoint_dict['model'].keys())\n",
    "# model.load_state_dict(state_dict['model'])\n",
    "\n",
    "filtered_dict = {k: v for k, v in checkpoint_dict['model'].items() if k in model_dict}\n",
    "unfiltered = {k: v for k, v in checkpoint_dict['model'].items() if k not in model_dict}\n",
    "\n",
    "\"\"\" Those are unused parameters \"\"\"\n",
    "print(\"\\n ****** Unused Parameters ****** \\n\")\n",
    "for k in unfiltered:\n",
    "    print(k, end='\\n')\n",
    "\n",
    "print(\"\\n ****** Used Parameters ****** \\n\")\n",
    "for k in filtered_dict:\n",
    "    print(k, end='\\n')\n",
    "\n",
    "model_dict.update(filtered_dict)\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=8, epochs=33, accum_iter=1, model='mae_vit_base_patch16', input_size=224, mask_ratio=0.8, norm_pix_loss=True, weight_decay=0.0001, lr=0.0004, blr=0.0002, min_lr=0.0, warmup_epochs=3, data_path='/datasets01/imagenet_full_size/061417/', output_dir=PosixPath('/checkpoint/berniehuang/experiments/56067384'), log_dir=PosixPath('/checkpoint/berniehuang/experiments/56067384'), device='cuda', seed=0, resume='', start_epoch=0, num_workers=10, pin_mem=True, world_size=64, local_rank=-1, dist_on_itp=False, dist_url='file:///checkpoint/berniehuang/experiments/674c1c0b0a3e4b54a9d7722b93d3b300_init', audio_exp=True, data_train='/checkpoint/berniehuang/ast/egs/audioset/data/datafiles/train_all.json', data_eval='/checkpoint/berniehuang/ast/egs/audioset/data/datafiles/eval_video.json', label_csv='/checkpoint/berniehuang/ast/egs/audioset/data/class_labels_indices.csv', freqm=0, timem=0, mixup=0, dataset='audioset', use_fbank=False, fbank_dir='/checkpoint/berniehuang/ast/egs/esc50/data/ESC-50-master/fbank', alpha=0.0, omega=1.0, mode=0, save_every_epoch=4, use_custom_patch=False, roll_mag_aug=True, split_pos=False, pos_trainable=False, use_nce=False, img_pt_ckpt='', load_video=False, sep_encoder=False, encoder_mode=0, decoder_mode=1, n_frm=6, depth_av=6, v_weight=1.0, mask_t_prob=0.7, mask_f_prob=0.3, mask_2d=False, video_only=False, cl=False, use_video_mae=False, ngpus=8, nodes=8, timeout=4320, job_dir=PosixPath('/checkpoint/berniehuang/experiments/%j'), partition='learnlab', use_volta32=True, comment='', gpu=0, rank=0, distributed=True, dist_backend='nccl')\n",
      "batch_size 8\n",
      "epochs 33\n",
      "accum_iter 1\n",
      "model mae_vit_base_patch16\n",
      "input_size 224\n",
      "mask_ratio 0.8\n",
      "norm_pix_loss True\n",
      "weight_decay 0.0001\n",
      "lr 0.0004\n",
      "blr 0.0002\n",
      "min_lr 0.0\n",
      "warmup_epochs 3\n",
      "data_path /datasets01/imagenet_full_size/061417/\n",
      "output_dir /checkpoint/berniehuang/experiments/56067384\n",
      "log_dir /checkpoint/berniehuang/experiments/56067384\n",
      "device cuda\n",
      "seed 0\n",
      "resume \n",
      "start_epoch 0\n",
      "num_workers 10\n",
      "pin_mem True\n",
      "world_size 64\n",
      "local_rank -1\n",
      "dist_on_itp False\n",
      "dist_url file:///checkpoint/berniehuang/experiments/674c1c0b0a3e4b54a9d7722b93d3b300_init\n",
      "audio_exp True\n",
      "data_train /checkpoint/berniehuang/ast/egs/audioset/data/datafiles/train_all.json\n",
      "data_eval /checkpoint/berniehuang/ast/egs/audioset/data/datafiles/eval_video.json\n",
      "label_csv /checkpoint/berniehuang/ast/egs/audioset/data/class_labels_indices.csv\n",
      "freqm 0\n",
      "timem 0\n",
      "mixup 0\n",
      "dataset audioset\n",
      "use_fbank False\n",
      "fbank_dir /checkpoint/berniehuang/ast/egs/esc50/data/ESC-50-master/fbank\n",
      "alpha 0.0\n",
      "omega 1.0\n",
      "mode 0\n",
      "save_every_epoch 4\n",
      "use_custom_patch False\n",
      "roll_mag_aug True\n",
      "split_pos False\n",
      "pos_trainable False\n",
      "use_nce False\n",
      "img_pt_ckpt \n",
      "load_video False\n",
      "sep_encoder False\n",
      "encoder_mode 0\n",
      "decoder_mode 1\n",
      "n_frm 6\n",
      "depth_av 6\n",
      "v_weight 1.0\n",
      "mask_t_prob 0.7\n",
      "mask_f_prob 0.3\n",
      "mask_2d False\n",
      "video_only False\n",
      "cl False\n",
      "use_video_mae False\n",
      "ngpus 8\n",
      "nodes 8\n",
      "timeout 4320\n",
      "job_dir /checkpoint/berniehuang/experiments/%j\n",
      "partition learnlab\n",
      "use_volta32 True\n",
      "comment \n",
      "gpu 0\n",
      "rank 0\n",
      "distributed True\n",
      "dist_backend nccl\n"
     ]
    }
   ],
   "source": [
    "args = checkpoint_dict['args']\n",
    "print(args)\n",
    "for k,v in vars(args).items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE torch.Size([1, 3, 2])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# a = torch.ones(1,1,4,4)\n",
    "# conv = nn.Conv2d(1, 5,(2,2), stride=(2,2))\n",
    "# b = conv(a)\n",
    "# print(a.shape)\n",
    "# print(b)\n",
    "\n",
    "# b = b.flatten(2)\n",
    "# print(b.shape)\n",
    "\n",
    "# b = b.transpose(1,2)\n",
    "# print(b.shape)\n",
    "\n",
    "# c = conv(a).flatten(2).transpose(1,2)\n",
    "# print(c.shape)\n",
    "\n",
    "b = torch.tensor([[1,2],[3,4],[5,6]]).unsqueeze(0)\n",
    "print(\"SHAPE\",b.shape)\n",
    "print(b)\n",
    "b = b.flatten(1)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "\n",
    "\n",
    "from einops import rearrange\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
